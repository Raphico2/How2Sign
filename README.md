# How2Sign
How2Sign is an innovative project focused on advancing the field of Sign Language Translation (SLT) from continuous sign language videos to spoken language sentences.
Traditional SLT methodologies have often been limited by small datasets and narrow domains, making our work a significant step forward.

Our project leverages the How2Sign dataset, which contains nearly 80 hours of instructional videos covering 10 different topics. This dataset is the first of its kind to provide a robust basis for SLT research, approved by the Carnegie Mellon University Institutional Review Board. We have trained a Transformer model on I3D video features, achieving a BLEU score of 8.03, which serves as a benchmark for future SLT efforts.

Our primary goal is to achieve a very high BLEU score and to explore various improvements, including input and output reformulation for the Transformer model. We publish the first open-source implementation of SLT for American Sign Language (ASL) to written English translation, complete with scripts for data preprocessing, model training, translation, and evaluation. This promotes reproducibility and allows for adaptation to other datasets.


